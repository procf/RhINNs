{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "876498a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000000224762404\n"
     ]
    }
   ],
   "source": [
    "from scipy.integrate import odeint\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def TEVP(params, Smax, tmin, tmax, gdot, N_d_ode):\n",
    "    G, sy, es, ep, kp, kn = np.array(params)\n",
    "\n",
    "    def odes(x, t):\n",
    "        S = x[0]\n",
    "        L = x[1]\n",
    "        dSdt = (G/(es + ep))*(-S + sy*L/Smax + (es + ep*L)*gdot/Smax)\n",
    "        dLdt = kp*(1 - L) - kn*L*gdot\n",
    "        return [dSdt, dLdt]\n",
    "\n",
    "    x0 = [0., 1.]\n",
    "    t = np.logspace(np.log10(tmin),np.log10(tmax), N_d_ode)\n",
    "    x = odeint(odes, x0 ,t)\n",
    "    SS = x[:,0]\n",
    "    L = x[:,1]\n",
    "    SR = gdot*np.ones(N_d_ode)\n",
    "    return t, SR, SS, L\n",
    "\n",
    "Time, ShearRate, ShearStress, Lambda = [], [], [], []\n",
    "N_g = 4\n",
    "gmin = .1\n",
    "gmax = 1.\n",
    "Smax = 20.847700\n",
    "tmin, tmax = 0.01, 100.\n",
    "N_d = 201\n",
    "\n",
    "shear = np.linspace(gmin, gmax, N_g)\n",
    "param_exact = [40, 10., 10., 5., 0.1, 0.3] #G, sy, es, ep, kp, kn\n",
    "\n",
    "for gdot in shear:\n",
    "    t, SR, SS, L = TEVP(param_exact, Smax, tmin, tmax, gdot, N_d)\n",
    "    Time = np.append(Time, t, axis=None)\n",
    "    ShearRate = np.append(ShearRate, SR, axis=None)\n",
    "    ShearStress = np.append(ShearStress, SS, axis=None)\n",
    "    Lambda = np.append(Lambda, L, axis=None)\n",
    "print(max(ShearStress))\n",
    "df = pd.DataFrame({\"Time\" : Time, \"ShearRate\" : ShearRate, \"ShearStress\" : ShearStress, \"Lambda\" : Lambda})\n",
    "df.to_excel(\"StartUp.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a4e051d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.optimize\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import random\n",
    "import itertools\n",
    "# Working with data\n",
    "DTYPE='float32'\n",
    "tf.keras.backend.set_floatx(DTYPE)\n",
    "SEED=42\n",
    "os.environ['PYTHONHASHSEED']=str(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "log10 = tf.experimental.numpy.log10\n",
    "#Data\n",
    "path = os.getcwd()\n",
    "files = os.listdir(path)\n",
    "df = {}\n",
    "\n",
    "CURVE, Shuffle = 'StartUp', True\n",
    "\n",
    "xlsx = pd.ExcelFile('{}.xlsx'.format(CURVE))\n",
    "df = pd.read_excel(xlsx, sheet_name=None)\n",
    "data = [[k,v] for k,v in df.items()] #k is the sheet name, v is the pandas df\n",
    "\n",
    "sample=0\n",
    "# sample data points\n",
    "x1_d = tf.reshape(tf.convert_to_tensor(data[sample][1]['Time'], dtype=DTYPE),(-1,1))\n",
    "x2_d = tf.reshape(tf.convert_to_tensor(data[sample][1]['ShearRate'], dtype=DTYPE), (-1,1))\n",
    "y1_d = tf.reshape(tf.convert_to_tensor(data[sample][1]['ShearStress'], dtype=DTYPE), (-1,1))\n",
    "y2_d = tf.reshape(tf.convert_to_tensor(data[sample][1]['Lambda'], dtype=DTYPE), (-1,1))\n",
    "\n",
    "tmin, tmax = np.min(x1_d), np.max(x1_d)\n",
    "xmin, xmax = np.min(x2_d), np.max(x2_d)\n",
    "lb = tf.constant([tmin, xmin], dtype=DTYPE).numpy()\n",
    "ub = tf.constant([tmax, xmax], dtype=DTYPE).numpy()\n",
    "\n",
    "#Collocation points\n",
    "N_d = 101\n",
    "t_f=np.logspace(np.log10(lb[0]), np.log10(ub[0]), N_d)\n",
    "x_f=np.linspace(lb[1], ub[1],4)\n",
    "X_f=list(itertools.product(t_f, x_f))\n",
    "X_f=tf.convert_to_tensor(X_f,dtype=DTYPE)\n",
    "\n",
    "#Initial points\n",
    "N_i = 50\n",
    "x1_f0 = tf.convert_to_tensor(np.linspace(lb[0], lb[0], N_i).reshape(-1,1),dtype=DTYPE)\n",
    "x2_f0 = tf.convert_to_tensor(np.linspace(lb[1], ub[1], N_i).reshape(-1,1),dtype=DTYPE)\n",
    "X_f0 = tf.concat([x1_f0, x2_f0], axis = 1)\n",
    "\n",
    "X_data = tf.concat([x1_d, x2_d], axis=1)\n",
    "y_data = tf.concat([y1_d], axis=1)  \n",
    "Xy_data = tf.concat([x1_d, x2_d, y1_d], axis=1)\n",
    "\n",
    "if Shuffle:\n",
    "    X_f = tf.random.shuffle(X_f)\n",
    "    X_f0 = tf.random.shuffle(X_f0)\n",
    "    Xy_data = tf.random.shuffle(Xy_data)\n",
    "    X_data = Xy_data[:,0:2]\n",
    "    y_data = Xy_data[:,2:3]\n",
    "\n",
    "\n",
    "in_dim, out_dim = 2, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83d38985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model architecture\n",
    "class PINN_NeuralNet(tf.keras.Model):\n",
    "    \"\"\" Set basic architecture of the PINN model.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "            output_dim=out_dim,\n",
    "            num_hidden_layers=4, \n",
    "            num_neurons_per_layer=20,\n",
    "            activation='tanh',\n",
    "            kernel_initializer='glorot_normal',\n",
    "            **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.output_dim = output_dim\n",
    "        self.lambd = tf.Variable(self.CM(CMC)[0]*tf.ones(self.CM(CMC)[1]), trainable=True, dtype=DTYPE,\n",
    "                                 constraint=lambda x: tf.clip_by_value(x, self.CM(CMC)[2], self.CM(CMC)[3]))    \n",
    "#         noise = tf.random.uniform(shape=self.lambd.shape, minval = -.05, maxval=.05, dtype=DTYPE, seed=42)\n",
    "#         self.lambd.assign_add(noise)\n",
    "\n",
    "        self.lambd_list = []\n",
    "    \n",
    "        # Define NN architecture\n",
    "        self.scale = tf.keras.layers.Lambda(\n",
    "            lambda x: 2.0*(x - lb)/(ub - lb) - 1.0)\n",
    "        self.hidden = [tf.keras.layers.Dense(num_neurons_per_layer,\n",
    "                             activation=tf.keras.activations.get(activation),\n",
    "                             kernel_initializer=kernel_initializer)\n",
    "                           for _ in range(self.num_hidden_layers)]\n",
    "        self.out = tf.keras.layers.Dense(output_dim)\n",
    "        \n",
    "    def CM(self, CMC):\n",
    "        if CMC == 1: #TEVP\n",
    "            num_param = 6\n",
    "            init = [1., 1., 1., 1., 1., 1.]\n",
    "            low = [1e-4, 1e-4, 1e-4, 1e-4, 1e-4, 1e-4]\n",
    "            high = [np.infty, np.infty, np.infty, np.infty, np.infty, np.infty]\n",
    "            return init, num_param, low, high\n",
    "    \n",
    "    def call(self, X):\n",
    "        \"\"\"Forward-pass through neural network.\"\"\"\n",
    "        Z = X\n",
    "#         Z = self.scale(X)\n",
    "        for i in range(self.num_hidden_layers):\n",
    "            Z = self.hidden[i](Z)\n",
    "        return self.out(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c719019d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINNSolver():\n",
    "    def __init__(self, model, X_f, X_f0):\n",
    "        self.model = model\n",
    "        # Store collocation points\n",
    "        self.x1_f = tf.gather(X_f,[0],axis=1)\n",
    "        self.x2_f = tf.gather(X_f,[1],axis=1)\n",
    "        # Initialize history of losses and global iteration counter\n",
    "        self.hist = []\n",
    "        self.iter = 0\n",
    "    \n",
    "    def fun_r(self, x1_f, x2_f, y1_pred, y2_pred, y1_t, y2_t):\n",
    "        G, sy, es, ep, kp, kn = [self.model.lambd[j] for j in range(self.model.CM(CMC)[1])]\n",
    "\n",
    "        R1 = y1_t - (G/(es + ep))*(-y1_pred + sy*y2_pred/Smax + (es + ep*y2_pred)*x2_f/Smax)\n",
    "        R2 = y2_t - (kp*(1. - y2_pred) - kn*y2_pred*x2_f)\n",
    "\n",
    "        return R1, R2\n",
    "    \n",
    "    def get_r(self):\n",
    "        \n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            tape.watch(self.x1_f)\n",
    "            y_pred = self.model(tf.concat([self.x1_f,self.x2_f],axis=1))\n",
    "            y1_pred = tf.gather(y_pred, [0], axis=1)\n",
    "            y2_pred = tf.gather(y_pred, [1], axis=1)\n",
    "        y1_t = tape.gradient(y1_pred, self.x1_f)\n",
    "        y2_t = tape.gradient(y2_pred, self.x1_f)\n",
    "\n",
    "        del tape\n",
    "        \n",
    "        return self.fun_r(self.x1_f, self.x2_f, y1_pred, y2_pred, y1_t, y2_t)\n",
    "    \n",
    "    def loss_fn(self, X_data, X_f0, y_data):        \n",
    "        R1, R2 = self.get_r()\n",
    "        Loss_eq = tf.reduce_mean(tf.square(R1)) + tf.reduce_mean(tf.square(R2))\n",
    "        \n",
    "        y_pred = self.model(X_data)\n",
    "        y_init = self.model(X_f0)\n",
    "        \n",
    "        Loss_init = tf.reduce_mean(tf.square(tf.gather(y_init, [0], axis=1) - 0.))\\\n",
    "        + tf.reduce_mean(tf.square(tf.gather(y_init, [1], axis=1) - 1.))\n",
    "        \n",
    "        Loss_data = tf.reduce_mean(tf.square(y_data - tf.gather(y_pred, [0], axis=1)))\n",
    "        Loss_max = tf.reduce_max(tf.stack([Loss_eq, Loss_init, Loss_data]))\n",
    "        \n",
    "        loss = Loss_eq + Loss_init + Loss_data#*Loss_max\n",
    "                \n",
    "        loss_frac = [Loss_eq, Loss_init, Loss_data]\n",
    "        return loss, loss_frac\n",
    "    \n",
    "    def get_grad(self, X_data, X_f0, y_data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(self.model.trainable_variables)\n",
    "            loss, loss_frac = self.loss_fn(X_data, X_f0, y_data)            \n",
    "        g = tape.gradient(loss, self.model.trainable_variables)\n",
    "        \n",
    "        del tape        \n",
    "        return loss, g, loss_frac\n",
    "    \n",
    "    def solve_with_TFoptimizer(self, optimizer, X_data, X_f0, y_data, N=1001):\n",
    "        \"\"\"This method performs a gradient descent type optimization.\"\"\"        \n",
    "        @tf.function\n",
    "        def train_step():\n",
    "            loss, grad_theta, loss_frac = self.get_grad(X_data, X_f0, y_data)            \n",
    "            # Perform gradient descent step\n",
    "            optimizer.apply_gradients(zip(grad_theta, self.model.trainable_variables))\n",
    "            return loss, loss_frac\n",
    "            \n",
    "    \n",
    "        loss_params = 100.\n",
    "        while loss_params >= 3.: #terminates when the maximum error in \n",
    "            #the recovered parameters gets below 3%\n",
    "            \n",
    "            loss, loss_frac = train_step()\n",
    "            self.loss_frac = loss_frac\n",
    "            self.current_loss = loss.numpy()\n",
    "            self.callback()\n",
    "            params = model.lambd\n",
    "            loss_params = tf.reduce_max(100.*tf.abs(params - param_exact)/param_exact)\n",
    "            \n",
    "        \n",
    "    def solve_with_ScipyOptimizer(self, X, u, method='L-BFGS-B', **kwargs):\n",
    "        \"\"\"This method provides an interface to solve the learning problem\n",
    "        using a routine from scipy.optimize.minimize.\n",
    "        (Tensorflow 1.xx had an interface implemented, which is not longer\n",
    "        supported in Tensorflow 2.xx.)\n",
    "        Type conversion is necessary since scipy-routines are written in Fortran\n",
    "        which requires 64-bit floats instead of 32-bit floats.\"\"\"\n",
    "        \n",
    "        def get_weight_tensor():\n",
    "            \"\"\"Function to return current variables of the model\n",
    "            as 1d tensor as well as corresponding shapes as lists.\"\"\"\n",
    "            \n",
    "            weight_list = []\n",
    "            shape_list = []\n",
    "            \n",
    "            # Loop over all variables, i.e. weight matrices, bias vectors and unknown parameters\n",
    "            for v in self.model.variables:\n",
    "                shape_list.append(v.shape)\n",
    "                weight_list.extend(v.numpy().flatten())\n",
    "                \n",
    "            weight_list = tf.convert_to_tensor(weight_list)\n",
    "            return weight_list, shape_list\n",
    "\n",
    "        x0, shape_list = get_weight_tensor()\n",
    "        \n",
    "        def set_weight_tensor(weight_list):\n",
    "            \"\"\"Function which sets list of weights\n",
    "            to variables in the model.\"\"\"\n",
    "            idx = 0\n",
    "            for v in self.model.variables:\n",
    "                vs = v.shape\n",
    "                \n",
    "                # Weight matrices\n",
    "                if len(vs) == 2:  \n",
    "                    sw = vs[0]*vs[1]\n",
    "                    new_val = tf.reshape(weight_list[idx:idx+sw],(vs[0],vs[1]))\n",
    "                    idx += sw\n",
    "                \n",
    "                # Bias vectors\n",
    "                elif len(vs) == 1:\n",
    "                    new_val = weight_list[idx:idx+vs[0]]\n",
    "                    idx += vs[0]\n",
    "                    \n",
    "                # Variables (in case of parameter identification setting)\n",
    "                elif len(vs) == 0:\n",
    "                    new_val = weight_list[idx]\n",
    "                    idx += 1\n",
    "                    \n",
    "                # Assign variables (Casting necessary since scipy requires float64 type)\n",
    "                v.assign(tf.cast(new_val, DTYPE))\n",
    "        \n",
    "        def get_loss_and_grad(w):\n",
    "            \"\"\"Function that provides current loss and gradient\n",
    "            w.r.t the trainable variables as vector. This is mandatory\n",
    "            for the LBFGS minimizer from scipy.\"\"\"\n",
    "            \n",
    "            # Update weights in model\n",
    "            set_weight_tensor(w)\n",
    "            # Determine value of \\phi and gradient w.r.t. \\theta at w\n",
    "            loss, grad, loss_frac = self.get_grad(X, X_f0, u)\n",
    "            \n",
    "            # Store current loss for callback function            \n",
    "            loss = loss.numpy().astype(np.float64)\n",
    "            self.current_loss = loss   \n",
    "            self.loss_frac = loss_frac\n",
    "            \n",
    "            # Flatten gradient\n",
    "            grad_flat = []\n",
    "            for g in grad:\n",
    "                grad_flat.extend(g.numpy().flatten())\n",
    "            \n",
    "            # Gradient list to array\n",
    "            grad_flat = np.array(grad_flat,dtype=np.float64)\n",
    "            \n",
    "            # Return value and gradient of \\phi as tuple\n",
    "            return loss, grad_flat\n",
    "        \n",
    "        return scipy.optimize.minimize(fun=get_loss_and_grad,\n",
    "                                       x0=x0,\n",
    "                                       jac=True,\n",
    "                                       method=method,\n",
    "                                       callback=self.callback,\n",
    "                                       **kwargs)\n",
    "        \n",
    "    def callback(self, xr=None):\n",
    "        lambd = self.model.lambd.numpy()        \n",
    "        self.model.lambd_list.append(lambd)\n",
    "        if self.iter % 5000 == 0:\n",
    "            tf.print('It {:05d}: loss = {:10.4e}, {}'.format(self.iter, self.current_loss, np.round(lambd, 3)))\n",
    "        self.hist.append(self.current_loss)\n",
    "        self.iter+=1\n",
    "    def plot_loss_history(self, ax=None):\n",
    "        if not ax:\n",
    "            fig = plt.figure(figsize=(7,5))\n",
    "            ax = fig.add_subplot(111)\n",
    "        ax.semilogy(range(len(self.hist)), self.hist,'k-')\n",
    "        ax.set_xlabel('$n_{epoch}$')\n",
    "        ax.set_ylabel('$\\\\phi^{n_{epoch}}$')\n",
    "        return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7503450a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CMC = 1\n",
    "model = PINN_NeuralNet()\n",
    "model.build(input_shape=(None,in_dim))\n",
    "solver = PINNSolver(model, X_f, X_f0)\n",
    "if 'runtime' in globals():\n",
    "    del runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06b095c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 00000: loss = 2.5555e+00, [0.99 0.99 1.01 1.01 0.99 0.99]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 10:49:03.819659: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 05000: loss = 4.7923e-02, [0.    2.124 1.608 1.594 0.634 0.046]\n",
      "It 10000: loss = 5.3920e-02, [0.    2.494 1.721 1.712 0.    0.   ]\n",
      "It 15000: loss = 8.5649e-03, [2.7320e+00 1.4458e+01 3.7400e-01 5.3300e-01 0.0000e+00 7.0000e-03]\n",
      "It 20000: loss = 4.9106e-03, [3.3130e+00 1.5781e+01 4.8400e-01 7.0800e-01 0.0000e+00 8.0000e-03]\n",
      "It 25000: loss = 3.6971e-03, [ 3.866 16.644  0.565  0.831  0.     0.023]\n",
      "It 30000: loss = 2.8389e-03, [ 4.318 16.091  0.596  0.949  0.     0.032]\n",
      "It 35000: loss = 2.7439e-03, [ 4.834 15.848  0.624  1.085  0.     0.03 ]\n",
      "It 40000: loss = 3.6698e-03, [ 5.584 16.002  0.642  1.202  0.     0.019]\n",
      "It 45000: loss = 3.0162e-03, [ 6.235 15.893  0.747  1.424  0.     0.019]\n",
      "It 50000: loss = 5.9970e-03, [ 7.006 15.615  0.83   1.65   0.     0.031]\n",
      "It 55000: loss = 2.3110e-03, [ 7.468 15.204  0.747  1.69   0.     0.029]\n",
      "It 60000: loss = 2.2523e-03, [ 8.008 15.246  0.781  1.861  0.     0.032]\n",
      "It 65000: loss = 1.9245e-03, [ 8.67  15.233  0.771  2.025  0.     0.032]\n",
      "It 70000: loss = 1.8016e-03, [ 9.244 15.169  0.782  2.201  0.     0.032]\n",
      "It 75000: loss = 1.9681e-03, [ 9.857 15.07   0.799  2.394  0.     0.034]\n",
      "It 80000: loss = 1.6529e-03, [10.505 14.939  0.815  2.599  0.     0.034]\n",
      "It 85000: loss = 1.6234e-03, [11.239 14.955  0.813  2.789  0.     0.031]\n",
      "It 90000: loss = 1.5570e-03, [11.969 14.959  0.866  3.02   0.     0.034]\n",
      "It 95000: loss = 1.7535e-03, [12.612 14.572  0.975  3.315  0.     0.041]\n",
      "It 100000: loss = 1.4901e-03, [13.514 14.374  0.918  3.488  0.     0.034]\n",
      "It 105000: loss = 1.3772e-03, [14.251 14.399  0.947  3.697  0.     0.036]\n",
      "It 110000: loss = 1.3110e-03, [15.036 14.291  0.988  3.937  0.     0.037]\n",
      "It 115000: loss = 1.2381e-03, [1.5849e+01 1.4138e+01 1.0280e+00 4.1840e+00 2.0000e-03 3.9000e-02]\n",
      "It 120000: loss = 1.1883e-03, [1.6706e+01 1.3967e+01 1.0710e+00 4.4450e+00 5.0000e-03 4.1000e-02]\n",
      "It 125000: loss = 1.1021e-03, [1.7607e+01 1.3784e+01 1.1170e+00 4.7200e+00 8.0000e-03 4.3000e-02]\n",
      "It 130000: loss = 1.0322e-03, [1.8548e+01 1.3592e+01 1.1660e+00 5.0080e+00 1.1000e-02 4.5000e-02]\n",
      "It 135000: loss = 9.6142e-04, [1.9522e+01 1.3393e+01 1.2200e+00 5.3070e+00 1.5000e-02 4.8000e-02]\n",
      "It 140000: loss = 8.9090e-04, [2.0525e+01 1.3188e+01 1.2780e+00 5.6140e+00 1.8000e-02 5.1000e-02]\n",
      "It 145000: loss = 8.6070e-04, [21.556 12.979  1.342  5.928  0.023  0.054]\n",
      "It 150000: loss = 7.5400e-04, [22.605 12.767  1.413  6.246  0.027  0.057]\n",
      "It 155000: loss = 6.9447e-04, [23.675 12.553  1.493  6.568  0.032  0.061]\n",
      "It 160000: loss = 6.1992e-04, [24.753 12.339  1.582  6.887  0.037  0.065]\n",
      "It 165000: loss = 5.5736e-04, [25.84  12.126  1.683  7.202  0.043  0.07 ]\n",
      "It 170000: loss = 4.9762e-04, [26.928 11.915  1.799  7.508  0.048  0.075]\n",
      "It 175000: loss = 4.4148e-04, [28.013 11.709  1.934  7.799  0.055  0.081]\n",
      "It 180000: loss = 3.8914e-04, [29.087 11.507  2.091  8.069  0.061  0.087]\n",
      "It 185000: loss = 3.4063e-04, [30.149 11.312  2.281  8.311  0.067  0.094]\n",
      "It 190000: loss = 2.9606e-04, [31.19  11.125  2.508  8.514  0.074  0.102]\n",
      "It 195000: loss = 2.6239e-04, [32.209 10.947  2.78   8.672  0.081  0.11 ]\n",
      "It 200000: loss = 2.2799e-04, [33.201 10.78   3.127  8.758  0.088  0.121]\n",
      "It 205000: loss = 2.1743e-04, [34.163 10.626  3.561  8.759  0.094  0.133]\n",
      "It 210000: loss = 1.5722e-04, [35.085 10.486  4.113  8.656  0.101  0.147]\n",
      "It 215000: loss = 1.2507e-04, [35.969 10.365  4.817  8.413  0.107  0.166]\n",
      "It 220000: loss = 9.5128e-05, [36.803 10.265  5.693  8.033  0.113  0.19 ]\n",
      "It 225000: loss = 6.6498e-05, [37.58  10.195  6.751  7.458  0.116  0.22 ]\n",
      "It 230000: loss = 3.5839e-05, [38.265 10.157  7.898  6.763  0.116  0.256]\n",
      "It 235000: loss = 1.5738e-05, [38.804 10.136  8.846  6.071  0.112  0.283]\n",
      "It 240000: loss = 7.7174e-06, [39.169 10.115  9.441  5.536  0.107  0.295]\n",
      "It 245000: loss = 2.6860e-06, [39.395 10.09   9.734  5.235  0.104  0.299]\n",
      "\n",
      "Runtime: 8.253 minutes\n"
     ]
    }
   ],
   "source": [
    "mode = 'TFoptimizer'\n",
    "# mode = 'ScipyOptimizer'\n",
    "\n",
    "lr = tf.keras.optimizers.schedules.PiecewiseConstantDecay([50000,100000],[1e-2,2e-3,1e-3])\n",
    "# lr = 1e-3\n",
    "N=2e6+1\n",
    "\n",
    "try:\n",
    "    runtime\n",
    "except NameError:\n",
    "    runtime = 0.\n",
    "\n",
    "if mode == 'TFoptimizer':\n",
    "    try:\n",
    "        t0 = time()\n",
    "        optim = tf.keras.optimizers.Adam(learning_rate = lr)\n",
    "        solver.solve_with_TFoptimizer(optim, X_data, X_f0, y_data, N=int(N))\n",
    "        runtime += (time()-t0)/60.\n",
    "        print('\\nRuntime: {:.3f} minutes'.format(runtime))\n",
    "    except KeyboardInterrupt:\n",
    "        runtime += (time()-t0)/60.\n",
    "        print('\\nRuntime: {:.3f} minutes'.format(runtime))\n",
    "\n",
    "elif mode == 'ScipyOptimizer':\n",
    "    try:\n",
    "        t0 = time()\n",
    "        solver.solve_with_ScipyOptimizer(X_data, y_data,\n",
    "                                        method='L-BFGS-B',\n",
    "                                        options={'maxiter': 1000000,'maxfun': 1000000,'maxcor': 1000,\n",
    "                                                              'maxls': 1000,'ftol' : 1.0 * np.finfo(float).eps})\n",
    "        runtime += (time()-t0)/60.\n",
    "        print('\\nRuntime: {:.3f} minutes'.format(runtime))\n",
    "    except KeyboardInterrupt:\n",
    "        runtime += (time()-t0)/60.\n",
    "        print('\\nRuntime: {:.3f} minutes'.format(runtime))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfe2d55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
